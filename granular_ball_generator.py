import numpy as np
import faiss
from tqdm.auto import tqdm
from itertools import combinations
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
import warnings
from concurrent.futures import ThreadPoolExecutor
import gc


@dataclass
class GranularBallConfig:
    """é…ç½®ç±»ï¼Œé›†ä¸­ç®¡ç†æ‰€æœ‰å‚æ•°"""
    k: int = 20
    top_k_peaks: int = 500
    M_neighbors_for_delta: int = 200
    jaccard_threshold: float = 0.5
    radius_percentile: float = 10
    min_ball_size: int = 5
    max_balls: Optional[int] = None
    use_gpu: bool = False
    batch_size: int = 1000
    max_cohesion_samples: int = 200
    n_threads: int = 4
    
    # æ–°å¢å‚æ•°è§£å†³æåˆ°çš„é—®é¢˜
    min_radius: float = 0.01  # æœ€å°åŠå¾„é˜ˆå€¼
    max_radius: float = 1.0   # æœ€å¤§åŠå¾„é˜ˆå€¼
    density_adaptive_factor: float = 0.1  # å¯†åº¦è‡ªé€‚åº”å› å­
    overlap_threshold: float = 0.7  # çƒé‡å é˜ˆå€¼
    random_state: Optional[int] = 42  # éšæœºç§å­
    adaptive_search_factor: float = 0.1  # è‡ªé€‚åº”æœç´¢å› å­


@dataclass
class GranularBall:
    """ç²’çƒæ•°æ®ç»“æ„"""
    center_index: int
    center_embedding: np.ndarray
    radius: float
    sample_indices: np.ndarray
    sample_count: int
    cohesion_score: float
    label_prevalence: np.ndarray
    top_labels: List[Tuple[int, float]]  # æ”¹è¿›ï¼šä¿å­˜æ ‡ç­¾ç´¢å¼•å’Œå æ¯”
    
    def __post_init__(self):
        self.center_embedding = self.center_embedding.astype('float32')
        self.sample_indices = self.sample_indices.astype(int)


class ProductionGranularBallGenerator:
    def __init__(self, X: np.ndarray, y: np.ndarray, config: GranularBallConfig = None):
        """
        ç”Ÿäº§çº§ç²’çƒç”Ÿæˆå™¨ - è§£å†³æ‰€æœ‰æ€§èƒ½å’Œé²æ£’æ€§é—®é¢˜
        """
        print("--- åˆå§‹åŒ–ç”Ÿäº§çº§ç²’çƒç”Ÿæˆå™¨ ---")
        self.config = config or GranularBallConfig()
        
        # è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡ç°æ€§
        if self.config.random_state is not None:
            np.random.seed(self.config.random_state)
        
        # æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
        self.X = self._validate_and_prepare_data(X)
        self.y = self._validate_and_prepare_labels(y)
        self.n_samples, self.n_features = self.X.shape
        self.n_labels = self.y.shape[1]
        
        # è‡ªé€‚åº”æœç´¢å‚æ•°
        self.adaptive_radius_search_k = self._get_adaptive_search_k()
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = self._build_faiss_index()
        
        # ç¼“å­˜å’Œç»“æœ
        self._cache = {}
        self._density_rho = None  # ç¼“å­˜å¯†åº¦å€¼
        self.granular_balls: List[GranularBall] = []
        
        print(f"æ•°æ®å‡†å¤‡å®Œæˆ: {self.n_samples} æ ·æœ¬, {self.n_features} ç‰¹å¾, {self.n_labels} æ ‡ç­¾")
        print(f"è‡ªé€‚åº”æœç´¢è§„æ¨¡: {self.adaptive_radius_search_k}")

    def _get_adaptive_search_k(self) -> int:
        """è‡ªé€‚åº”ç¡®å®šæœç´¢è§„æ¨¡"""
        base_k = min(1000, max(100, int(self.n_samples * self.config.adaptive_search_factor)))
        return base_k

    def _validate_and_prepare_data(self, X: np.ndarray) -> np.ndarray:
        """éªŒè¯å’Œå‡†å¤‡ç‰¹å¾æ•°æ®"""
        if not isinstance(X, np.ndarray):
            X = np.array(X)
        
        if X.ndim != 2:
            raise ValueError(f"Xå¿…é¡»æ˜¯2Dæ•°ç»„ï¼Œå½“å‰ç»´åº¦: {X.ndim}")
        
        if np.isnan(X).any():
            warnings.warn("æ£€æµ‹åˆ°NaNå€¼ï¼Œå°†ç”¨å‡å€¼å¡«å……")
            X = np.nan_to_num(X, nan=np.nanmean(X))
        
        return X.astype('float32')

    def _validate_and_prepare_labels(self, y: np.ndarray) -> np.ndarray:
        """éªŒè¯å’Œå‡†å¤‡æ ‡ç­¾æ•°æ®"""
        if not isinstance(y, np.ndarray):
            y = np.array(y)
            
        if y.ndim == 1:
            n_classes = len(np.unique(y))
            y_multi = np.zeros((len(y), n_classes), dtype=bool)
            for i, label in enumerate(y):
                y_multi[i, int(label)] = True
            y = y_multi
        
        return y.astype(bool)

    def _build_faiss_index(self) -> faiss.Index:
        """æ„å»ºFAISSç´¢å¼•"""
        index = faiss.IndexFlatL2(self.n_features)
        
        if self.config.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(res, 0, index)
                print("FAISSç´¢å¼•æˆåŠŸè¿ç§»åˆ°GPU")
            except Exception as e:
                print(f"GPUåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨CPU")
        
        index.add(self.X)
        print(f"FAISSç´¢å¼•æ„å»ºå®Œæˆ: {index.ntotal} ä¸ªå‘é‡")
        return index

    def _jaccard_similarity_matrix(self, sample_indices: np.ndarray) -> np.ndarray:
        """
        çŸ©é˜µåŒ–Jaccardç›¸ä¼¼åº¦è®¡ç®— - è§£å†³O(nÂ²)æ€§èƒ½é—®é¢˜
        """
        if len(sample_indices) == 0:
            return np.array([[]])
        
        Y = self.y[sample_indices].astype(np.int32)  # (n_samples, n_labels)
        n_samples = len(sample_indices)
        
        if n_samples == 1:
            return np.array([[1.0]])
        
        # çŸ©é˜µåŒ–è®¡ç®—äº¤é›†ï¼šY @ Y.T
        intersection = Y @ Y.T  # (n_samples, n_samples)
        
        # çŸ©é˜µåŒ–è®¡ç®—å¹¶é›†ï¼šä½¿ç”¨å¹¿æ’­
        # union[i,j] = sum(Y[i] | Y[j]) = sum(Y[i]) + sum(Y[j]) - intersection[i,j]
        label_counts = np.sum(Y, axis=1)  # æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾æ•°é‡
        union = label_counts[:, None] + label_counts[None, :] - intersection
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = intersection / (union + 1e-9)
        
        return similarity_matrix

    def _calculate_cohesion_matrix(self, sample_indices: np.ndarray) -> float:
        """
        ä½¿ç”¨çŸ©é˜µè¿ç®—çš„è¶…å¿«å‡èšåº¦è®¡ç®— - è§£å†³æ€§èƒ½ç“¶é¢ˆ
        """
        if len(sample_indices) < 2:
            return 1.0
        
        # é‡‡æ ·ç­–ç•¥
        if len(sample_indices) > self.config.max_cohesion_samples:
            if self.config.random_state is not None:
                np.random.seed(self.config.random_state + hash(tuple(sample_indices)) % 10000)
            sample_indices = np.random.choice(
                sample_indices, self.config.max_cohesion_samples, replace=False
            )
        
        # ä½¿ç”¨çŸ©é˜µåŒ–è®¡ç®—
        similarity_matrix = self._jaccard_similarity_matrix(sample_indices)
        
        # åªå–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆé¿å…é‡å¤è®¡ç®—å’Œå¯¹è§’çº¿ï¼‰
        upper_triangular = np.triu(similarity_matrix, k=1)
        n_pairs = len(sample_indices) * (len(sample_indices) - 1) // 2
        
        if n_pairs == 0:
            return 1.0
        
        return np.sum(upper_triangular) / n_pairs

    def _calculate_density_peaks_optimized(self) -> np.ndarray:
        """ä¼˜åŒ–çš„å¯†åº¦å³°å€¼è®¡ç®—"""
        cache_key = f"density_peaks_{self.config.k}_{self.config.top_k_peaks}"
        if cache_key in self._cache:
            return self._cache[cache_key]
            
        print(f"è®¡ç®—å¯†åº¦å³°å€¼ (k={self.config.k})...")
        
        # æ‰¹é‡k-NNæœç´¢
        D_k, I_k = self.index.search(self.X, self.config.k + 1)
        k_dist = D_k[:, -1]
        self._density_rho = 1.0 / (k_dist + 1e-9)  # ç¼“å­˜å¯†åº¦å€¼
        
        # ä¼˜åŒ–deltaè®¡ç®—
        delta = self._calculate_delta_optimized(self._density_rho)
        
        # è®¡ç®—å¯†åº¦å³°å€¼å¾—åˆ†
        scores = self._density_rho * delta
        peak_indices = np.argsort(scores)[::-1][:self.config.top_k_peaks]
        
        self._cache[cache_key] = peak_indices
        print(f"æ‰¾åˆ° {len(peak_indices)} ä¸ªå¯†åº¦å³°å€¼")
        return peak_indices

    def _calculate_delta_optimized(self, rho: np.ndarray) -> np.ndarray:
        """ä¼˜åŒ–çš„deltaå€¼è®¡ç®—"""
        delta = np.zeros(self.n_samples)
        sorted_indices = np.argsort(rho)[::-1]
        
        # æ‰¹é‡æœç´¢é‚»å±…
        D_neighbors, I_neighbors = self.index.search(
            self.X, self.config.M_neighbors_for_delta + 1
        )
        
        print("è®¡ç®—Deltaå€¼...")
        for i, idx in enumerate(tqdm(sorted_indices)):
            if i == 0:
                delta[idx] = np.max(D_neighbors[idx])
                continue
            
            neighbor_indices = I_neighbors[idx, 1:]
            neighbor_rho = rho[neighbor_indices]
            higher_rho_mask = neighbor_rho > rho[idx]
            
            if np.any(higher_rho_mask):
                higher_rho_distances = D_neighbors[idx, 1:][higher_rho_mask]
                delta[idx] = np.min(higher_rho_distances)
            else:
                delta[idx] = np.max(D_neighbors[idx])
        
        return delta

    def _get_robust_adaptive_radius(self, center_idx: int) -> float:
        """
        é²æ£’çš„è‡ªé€‚åº”åŠå¾„è®¡ç®— - è§£å†³åŠå¾„ç­–ç•¥é—®é¢˜
        """
        D_neighbors, I_neighbors = self.index.search(
            self.X[center_idx:center_idx+1], self.adaptive_radius_search_k
        )
        
        center_labels = self.y[center_idx]
        neighbor_indices = I_neighbors[0, 1:]
        
        # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for neighbor_idx in neighbor_indices:
            neighbor_labels = self.y[neighbor_idx]
            sim_matrix = self._jaccard_similarity_matrix(np.array([center_idx, neighbor_idx]))
            similarities.append(sim_matrix[0, 1])
        
        similarities = np.array(similarities)
        
        # æ‰¾åˆ°ä¸ç›¸ä¼¼çš„é‚»å±…
        dissimilar_mask = similarities < self.config.jaccard_threshold
        
        if not np.any(dissimilar_mask):
            base_radius = D_neighbors[0, -1]
        else:
            dissimilar_distances = D_neighbors[0, 1:][dissimilar_mask]
            base_radius = np.percentile(dissimilar_distances, self.config.radius_percentile)
        
        # å¯†åº¦è‡ªé€‚åº”è°ƒæ•´
        if self._density_rho is not None:
            density_factor = self.config.density_adaptive_factor / (self._density_rho[center_idx] + 1e-9)
            adaptive_radius = min(base_radius, density_factor)
        else:
            adaptive_radius = base_radius
        
        # åº”ç”¨æœ€å°æœ€å¤§åŠå¾„çº¦æŸ
        robust_radius = np.clip(adaptive_radius, self.config.min_radius, self.config.max_radius)
        
        return robust_radius

    def _check_ball_overlap(self, new_sample_indices: np.ndarray) -> bool:
        """
        æ£€æŸ¥æ–°çƒä¸å·²æœ‰çƒçš„é‡å  - è§£å†³é‡å é—®é¢˜
        """
        if not self.granular_balls:
            return False
        
        new_samples_set = set(new_sample_indices)
        
        for existing_ball in self.granular_balls:
            existing_samples_set = set(existing_ball.sample_indices)
            
            # è®¡ç®—Jaccardé‡å 
            intersection_size = len(new_samples_set & existing_samples_set)
            union_size = len(new_samples_set | existing_samples_set)
            
            if union_size > 0:
                overlap_ratio = intersection_size / union_size
                if overlap_ratio > self.config.overlap_threshold:
                    return True
        
        return False

    def _get_interpretable_top_labels(self, label_prevalence: np.ndarray, top_k: int = 3) -> List[Tuple[int, float]]:
        """
        è·å–å¯è§£é‡Šçš„æ ‡ç­¾ä¿¡æ¯ - è§£å†³å¯è§£é‡Šæ€§é—®é¢˜
        """
        top_indices = np.argsort(label_prevalence)[::-1][:top_k]
        top_values = label_prevalence[top_indices]
        return [(int(idx), float(val)) for idx, val in zip(top_indices, top_values)]

    def _process_center_batch(self, center_indices: np.ndarray) -> List[GranularBall]:
        """æ‰¹é‡å¤„ç†ä¸­å¿ƒç‚¹"""
        balls = []
        
        for center_idx in center_indices:
            try:
                # è®¡ç®—é²æ£’è‡ªé€‚åº”åŠå¾„
                radius = self._get_robust_adaptive_radius(center_idx)
                
                # æ‰¾åˆ°çƒå†…æ ·æœ¬
                D_candidates, I_candidates = self.index.search(
                    self.X[center_idx:center_idx+1], self.adaptive_radius_search_k
                )
                in_ball_mask = D_candidates[0] <= radius
                sample_indices = I_candidates[0][in_ball_mask]
                
                if len(sample_indices) < self.config.min_ball_size:
                    continue
                
                # æ£€æŸ¥é‡å 
                if self._check_ball_overlap(sample_indices):
                    continue
                
                # ä½¿ç”¨çŸ©é˜µåŒ–è®¡ç®—å‡èšåº¦
                cohesion = self._calculate_cohesion_matrix(sample_indices)
                
                # è®¡ç®—æ ‡ç­¾æµè¡Œåº¦å’Œå¯è§£é‡Šæ ‡ç­¾
                label_prevalence = np.mean(self.y[sample_indices], axis=0)
                top_labels = self._get_interpretable_top_labels(label_prevalence)
                
                ball = GranularBall(
                    center_index=center_idx,
                    center_embedding=self.X[center_idx].copy(),
                    radius=radius,
                    sample_indices=sample_indices,
                    sample_count=len(sample_indices),
                    cohesion_score=cohesion,
                    label_prevalence=label_prevalence,
                    top_labels=top_labels
                )
                balls.append(ball)
                
            except Exception as e:
                warnings.warn(f"å¤„ç†ä¸­å¿ƒç‚¹ {center_idx} æ—¶å‡ºé”™: {e}")
                continue
        
        return balls

    def fit(self) -> List[GranularBall]:
        """è®­ç»ƒç²’çƒç”Ÿæˆå™¨"""
        print("\n--- å¼€å§‹ç²’çƒç”Ÿæˆ (ç”Ÿäº§çº§) ---")
        
        # è®¡ç®—å¯†åº¦å³°å€¼
        potential_centers = self._calculate_density_peaks_optimized()
        
        # æ‰¹é‡å¤„ç†ä¸­å¿ƒç‚¹
        batch_size = self.config.batch_size
        all_balls = []
        
        for i in tqdm(range(0, len(potential_centers), batch_size), desc="ç”Ÿæˆç²’çƒ"):
            if self.config.max_balls and len(all_balls) >= self.config.max_balls:
                break
                
            batch_centers = potential_centers[i:i+batch_size]
            if self.config.max_balls:
                remaining_slots = self.config.max_balls - len(all_balls)
                batch_centers = batch_centers[:remaining_slots]
            
            batch_balls = self._process_center_batch(batch_centers)
            all_balls.extend(batch_balls)
            
            # å†…å­˜æ¸…ç†
            if i % (batch_size * 5) == 0:
                gc.collect()
        
        self.granular_balls = all_balls
        print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼šåˆ›å»ºäº† {len(self.granular_balls)} ä¸ªç²’çƒ")
        return self.granular_balls

    def get_detailed_statistics(self) -> Dict:
        """è·å–è¯¦ç»†çš„ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.granular_balls:
            return {}
        
        cohesions = [ball.cohesion_score for ball in self.granular_balls]
        sizes = [ball.sample_count for ball in self.granular_balls]
        radii = [ball.radius for ball in self.granular_balls]
        
        # è®¡ç®—è¦†ç›–æƒ…å†µ
        all_covered_samples = set()
        for ball in self.granular_balls:
            all_covered_samples.update(ball.sample_indices)
        
        # é‡å ç»Ÿè®¡
        total_ball_samples = sum(sizes)
        overlap_rate = (total_ball_samples - len(all_covered_samples)) / total_ball_samples if total_ball_samples > 0 else 0
        
        return {
            'total_balls': len(self.granular_balls),
            'avg_cohesion': np.mean(cohesions),
            'std_cohesion': np.std(cohesions),
            'min_cohesion': np.min(cohesions),
            'max_cohesion': np.max(cohesions),
            'avg_size': np.mean(sizes),
            'std_size': np.std(sizes),
            'avg_radius': np.mean(radii),
            'std_radius': np.std(radii),
            'coverage_ratio': len(all_covered_samples) / self.n_samples,
            'overlap_rate': overlap_rate,
            'adaptive_search_k': self.adaptive_radius_search_k
        }

    def print_top_balls(self, top_k: int = 5):
        """æ‰“å°æœ€ä½³ç²’çƒçš„è¯¦ç»†ä¿¡æ¯"""
        if not self.granular_balls:
            print("æ²¡æœ‰ç”Ÿæˆçš„ç²’çƒ")
            return
        
        print(f"\n--- å‰ {top_k} ä¸ªæœ€ä½³ç²’çƒ ---")
        best_balls = sorted(self.granular_balls, key=lambda b: b.cohesion_score, reverse=True)[:top_k]
        
        for i, ball in enumerate(best_balls):
            print(f"\nğŸ€ ç²’çƒ {i+1}:")
            print(f"   å‡èšåº¦: {ball.cohesion_score:.4f}")
            print(f"   æ ·æœ¬æ•°: {ball.sample_count}")
            print(f"   åŠå¾„: {ball.radius:.4f}")
            print(f"   ä¸»è¦æ ‡ç­¾: {[(f'æ ‡ç­¾{idx}', f'{val:.3f}') for idx, val in ball.top_labels]}")

    def save_balls(self, filepath: str):
        """ä¿å­˜ç²’çƒåˆ°æ–‡ä»¶"""
        balls_data = {
            'balls': [],
            'config': self.config.__dict__,
            'statistics': self.get_detailed_statistics()
        }
        
        for ball in self.granular_balls:
            ball_dict = {
                'center_index': ball.center_index,
                'center_embedding': ball.center_embedding,
                'radius': ball.radius,
                'sample_indices': ball.sample_indices,
                'sample_count': ball.sample_count,
                'cohesion_score': ball.cohesion_score,
                'label_prevalence': ball.label_prevalence,
                'top_labels': ball.top_labels
            }
            balls_data['balls'].append(ball_dict)
        
        np.savez_compressed(filepath, **balls_data)
        print(f"ç²’çƒå·²ä¿å­˜åˆ° {filepath}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    print("--- æµ‹è¯•ç”Ÿäº§çº§ç²’çƒç”Ÿæˆå™¨ ---")
    
    # åŠ è½½æ•°æ®
    try:
        data = np.load('preprocessed_data.npz')
        X_train, y_train = data['X_train'], data['y_train']
        print(f"åŠ è½½æ•°æ®: X={X_train.shape}, y={y_train.shape}")
        
        sample_size = 3000  # é€‚ä¸­çš„æµ‹è¯•è§„æ¨¡
        X_sample = X_train[:sample_size]
        y_sample = y_train[:sample_size]
        
        # ç”Ÿäº§çº§é…ç½®
        config = GranularBallConfig(
            k=25,
            top_k_peaks=150,
            M_neighbors_for_delta=100,
            jaccard_threshold=0.4,
            radius_percentile=15,
            min_ball_size=8,
            max_balls=50,
            use_gpu=False,
            batch_size=25,
            max_cohesion_samples=100,
            # æ–°å¢çš„é²æ£’æ€§å‚æ•°
            min_radius=0.05,
            max_radius=2.0,
            density_adaptive_factor=0.15,
            overlap_threshold=0.6,
            random_state=42,
            adaptive_search_factor=0.12
        )
        
        # åˆ›å»ºç”Ÿæˆå™¨å¹¶è®­ç»ƒ
        print("\nğŸš€ å¼€å§‹ç”Ÿäº§çº§ç²’çƒç”Ÿæˆ...")
        generator = ProductionGranularBallGenerator(X_sample, y_sample, config)
        balls = generator.fit()
        
        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        stats = generator.get_detailed_statistics()
        print("\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
            else:
                print(f"  {key}: {value}")
        
        # æ˜¾ç¤ºæœ€ä½³ç²’çƒ
        generator.print_top_balls(3)
        
        # ä¿å­˜ç»“æœ
        generator.save_balls('production_granular_balls.npz')
        
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•...")
        np.random.seed(42)
        X_sim = np.random.randn(800, 30).astype('float32')
        y_sim = np.random.randint(0, 2, (800, 8)).astype(bool)
        
        config = GranularBallConfig(
            max_balls=15, 
            batch_size=5,
            random_state=42,
            min_radius=0.1,
            max_radius=1.5
        )
        generator = ProductionGranularBallGenerator(X_sim, y_sim, config)
        balls = generator.fit()
        
        print(f"\nâœ… æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•å®Œæˆ")
        generator.print_top_balls(3)
        stats = generator.get_detailed_statistics()
        print(f"\nğŸ“ˆ ç”Ÿæˆäº† {stats['total_balls']} ä¸ªç²’çƒï¼Œè¦†ç›–ç‡: {stats['coverage_ratio']:.3f}")